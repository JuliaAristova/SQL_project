Data Normalization in Databases

Data normalization is the process of organizing data in a database to reduce redundancy and improve data integrity.
It involves dividing large tables into smaller, more structured ones and defining relationships between them.

Why Normalize Data?
Eliminate Data Redundancy – Prevents storing duplicate data.
Ensure Data Integrity – Maintains consistency across the database.
Improve Query Performance – Optimizes retrieval and updates.
Simplify Maintenance – Easier to modify without affecting other data.

1NF - First normal form: no repeating groups; all tables are two-dimensional/
2NF - Second normal form: 1NF plus each data element is identifies by one corresponding unique identifier - a primary key -
      that is not a composite and threrefore cannot be subdivided into smaller bits of data.
3NF - Third normal form: 2NF plus all tables contain no data oter thab that which describes the intent of the primary key;
      extraneous data is placed in separate tables.
BCNF- Boyce-Codd: A slightly mofigied version of 3NF designed to eliminate structures that might allow some rare logical inconsistencies 
      to appear in the data
4NF - Forth normal form: BCNF plus additional logic to ensure that every multivalued depenency is dependent on a superley
5NF - Fifth normal form: 4NF plus every join dependency for the table is a result of the candidate key.

1NF, 2NF - heavy read-only use cases (less joins to retrive the required data ==> faster). 
Good for the large systems where there is no need to add or update data (Data warehouses)
Most transaction-based database applications are designed to be 3NF (3NF - starting point for most data)

Most data enters an organization through a transaction-based systemn (3NF) and it then moved into a data warehouse or data lake for analysis (a 2NF or 1NF)

Big data" systems are often described ad denormalized.
